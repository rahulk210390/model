package com.pred.mqstream

import java.io.{InputStreamReader, BufferedReader, InputStream}
import java.net.Socket

import org.apache.spark.{SparkConf, Logging}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.receiver.Receiver

object SQLHiveContextSingleton {
    @transient private var instance: HiveContext = _
    def getInstance(sparkContext: SparkContext): HiveContext = {
        synchronized {
              if (instance == null ) {
                instance = new HiveContext(sparkContext)
            }
            instance
        }
    }
}
object MQStream {
  def main(args: Array[String]) {
           // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("CustomReceiver")
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create a input stream with the custom receiver on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val lines = ssc.receiverStream(new CustomMQReceiver("113.130.226.63", 1419, "ADSL", "TO.ADSL", "ISPGW_RESP_TO_VC_REALTIME"))
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print
   val schema = new StructType().add("word", StringType, true).add("count", IntegerType, false)
				wordCounts.foreachRDD( rdd => {
        println("Processing mydata RDD")
        val sqlContext = SQLHiveContextSingleton.getInstance(rdd.sparkContext)
        val smDF = sqlContext.createDataFrame(rdd, schema)
        smDF.registerTempTable("sm")
        sqlContext.sql("use billinganalytics")
        val smTrgPart = sqlContext.sql("insert into table billinganalytics.streamingmq select * from sm")
        smTrgPart.write.mode(SaveMode.Append).saveAsTable("streamingmq")
    ssc.start()
    ssc.awaitTermination()
  }
}